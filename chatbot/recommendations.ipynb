{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "english-stick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "devoted-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"UCoursera_Courses.csv\")\n",
    "data.drop([\"Unnamed: 0\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "coral-measurement",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Vanshika\n",
      "[nltk_data]     Nehra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Vanshika\n",
      "[nltk_data]     Nehra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "def remove(myString):\n",
    "  myString = re.sub(r\"[\\n\\t]*\", \"\", myString)\n",
    "  myString = re.sub(r'[.,\"\\'-?:!;]', '', myString)\n",
    "  return myString\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stopwords=nltk.corpus.stopwords.words('english')\n",
    "def clean_text(txt):\n",
    "    txt=\"\".join([c for c in txt if c not in string.punctuation])\n",
    "    tokens=re.split('\\W+',txt)\n",
    "    txt=[word for word in tokens if word not in stopwords]\n",
    "    return txt\n",
    "\n",
    "nltk.download('wordnet')\n",
    "wn=nltk.WordNetLemmatizer()\n",
    "def lemm(token_txt):\n",
    "    text=[wn.lemmatize(word) for word in token_txt]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proud-onion",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['course_title_l']= data['course_title'].apply(lambda x: remove(x))\n",
    "data['course_organization_l']= data['course_organization'].apply(lambda x: remove(x))\n",
    "data['course_title_l']=data['course_title_l'].apply(lambda x:clean_text(x))\n",
    "data['course_organization_l']= data['course_organization_l'].apply(lambda x: clean_text(x))\n",
    "data['course_title_l']=data['course_title_l'].apply(lambda x: lemm(x))\n",
    "data['course_organization_l']=data['course_organization_l'].apply(lambda x: lemm(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wicked-headline",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['course_Certificate_type']=data['course_Certificate_type'].mask(data['course_Certificate_type']=='PROFESSIONAL CERTIFICATE','PROFESSIONAL_CERTIFICATE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "certain-works",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_soup(x):\n",
    "    return ' '.join(x['course_title_l']) + ' ' + ' '.join(x['course_organization_l']) + ' ' + x['course_Certificate_type'] + ' ' + x['course_difficulty']\n",
    "\n",
    "data['soup'] = data.apply(create_soup, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dress-council",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_course():\n",
    "    course = input(\"What Course are you interested in?\")\n",
    "    courses = \" \".join([\"\".join(n.split()) for n in course.lower().split(',')])\n",
    "    return course\n",
    "\n",
    "def get_searchTerms():\n",
    "    searchTerms = \"\" \n",
    "    courses = get_course()\n",
    "    if courses != 'skip':\n",
    "        searchTerms = courses\n",
    "    return searchTerms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "greenhouse-cream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recommendation(data=data):\n",
    "    searchTerms = get_searchTerms()\n",
    "    searchTerms = remove(searchTerms)\n",
    "    searchTerms = clean_text(searchTerms)\n",
    "    searchTerms = lemm(searchTerms)\n",
    "    new_row = data.iloc[-1,:].copy()\n",
    "    new_row.iloc[-1] = \" \".join(searchTerms)\n",
    "    data = data.append(new_row)\n",
    "    count = CountVectorizer(stop_words='english')\n",
    "    count_matrix = count.fit_transform(data['soup'])  \n",
    "    cosine_sim2 = cosine_similarity(count_matrix, count_matrix) #getting a similarity matrix\n",
    "    sim_scores = list(enumerate(cosine_sim2[-1,:]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "    ranked_titles = []\n",
    "    for i in range(1, 4):\n",
    "        indx = sim_scores[i][0]\n",
    "        if (sim_scores[i][1] != 0):\n",
    "            ranked_titles.append([data['course_title'].iloc[indx], data['course_rating'].iloc[indx], data['course_students_enrolled'].iloc[indx]])\n",
    "#     ranked_titles= sorted(ranked_titles, key=itemgetter(1), reverse=True)\n",
    "    if len(ranked_titles)==0:\n",
    "        return -1\n",
    "    else:\n",
    "        return ranked_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "make_recommendation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "collective-yemen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey There!\n",
      "\n",
      "What Course are you interested in?Chinese\n",
      "['Chinese for HSK 1', 4.8, '22k']\n",
      "['Learn Chinese: HSK Test Preparation', 4.8, '43k']\n",
      "['Learn Mandarin Chinese', 4.8, '39k']\n",
      "Do you want to see more type of courses? (Yes/No)y\n",
      "\n",
      "What Course are you interested in?Cloud Computing\n",
      "['Cloud Computing Basics (Cloud 101)', 4.5, '37k']\n",
      "['Introduction to Cloud Computing', 4.6, '2.6k']\n",
      "['Cloud Computing', 4.4, '110k']\n",
      "Do you want to see more type of courses? (Yes/No)y\n",
      "\n",
      "What Course are you interested in?finance\n",
      "['Supply Chain Finance and Blockchain Technology', 4.6, '4.9k']\n",
      "['Behavioral Finance', 4.4, '55k']\n",
      "['Essentials of Corporate Finance', 4.6, '54k']\n",
      "Do you want to see more type of courses? (Yes/No)n\n",
      "Hope you have your courses. See you Again!\n"
     ]
    }
   ],
   "source": [
    "print(\"Hey There!\")\n",
    "\n",
    "L=[]\n",
    "while(True):\n",
    "    print()\n",
    "    L = make_recommendation()\n",
    "    if L !=-1:\n",
    "        for i in range(len(L)):\n",
    "            print(L[i])\n",
    "    choice = input(\"Do you want to see more type of courses? (Yes/No)\")\n",
    "    if ((choice.lower() == \"yes\") or (choice.lower() == \"y\")):\n",
    "        continue;\n",
    "    else:\n",
    "        print(\"Hope you have your courses. See you Again!\")\n",
    "        break;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "roman-opportunity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
